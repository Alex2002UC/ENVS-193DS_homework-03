---
title: "HW-3"
author: "Alexa Andrade"
date: 06-01-25
format: html
toc: true
editor: visual
excecute:
 warning: false
 message: false
---

```{r}
#Reading in all the packages 
library(tidyverse)
library(here)
library(gt)
library(flextable)
library(janitor)
library(readxl)
library(tidyverse)
library(here)
library(readxl)
library(dplyr)
library(flextable)

options(warn = -1) #Gets rid of all warning messages 



```
##link to github repositroy: https://github.com/Alex2002UC/ENVS-193DS_homework-03 

\## Part 2. Problems

### Problem 1. Personal data (30 points)

#### a. Data summarizing (5 points)

I could take and compare the mean time I spent cooking on school days and non-school days because I tend to more homework and have to attend classes during the week, so I think I would spend more time cooking during the weekend.

#### b. Visualization (10 points)

```{r Caclulating Summary Statistics}

Cooking_clean <- read_xlsx(here("Data", "Cooking_timeF.xlsx")) |> #Reading in my data and putting it into a new object name
clean_names() #Running the function clean_names to standarize my column names

Cooking_summary <- Cooking_clean |> #Creating a summary object
  group_by(school_day) |>  #grouping the statistics by Y or N for school day
  summarise(
    mean = mean(minutes_spent_cooking_min),        # Calculate mean time spent cooking
    n = n(),                              # Count number of observations
    sd = sd(minutes_spent_cooking_min),            # Calculate standard deviation
    se = sd / sqrt(n),                   # Calculate standard error
    ci_lower = mean - qt(0.975, df = n - 1) * se,  # 95% CI lower bound
    ci_upper = mean + qt(0.975, df = n - 1) * se   # 95% CI upper bound
  ) |>
  mutate(
    across(c(mean, sd, se, ci_lower, ci_upper), ~round(.x, 1))  # Round values to first decimal point 
  ) |>
  select(school_day, mean, sd, se, ci_lower, ci_upper)  # Select final columns I want shown 
```

```{r Visualizing Data}

ggplot(data = Cooking_clean, aes(x = school_day, y = minutes_spent_cooking_min, color = school_day)) + #Setting up data for plotting
  geom_jitter(width = 0.2, height = 0, alpha = 0.6, shape = 21, 
              aes(fill = school_day, color = "black")) +  # Adds raw data points with jitter plot, shows the underlying data 
  
  geom_errorbar(data = Cooking_summary, #Adds error bars determined by SE
                aes(x = school_day, ymin = mean - se, ymax = mean + se),
                width = 0.2, #width of error bars
                inherit.aes = FALSE,
                color = "black") +  # Error bars for mean ± SE, colored black
  
  geom_point(data = Cooking_summary, #Adds mean points on top of the error bars 
             aes(x = school_day, y = mean),
             inherit.aes = FALSE,
             color = "black", 
             size = 3) +  # Size of the mean points
  
  labs(x = "Status of Day",  #Adding Axis labels and subtitle
       y = "Average Time Spent Cooking(min)", 
       title = "Mean time spent cooking During the Week",
       subtitle = "Alexa Andrade") +
  
scale_x_discrete(labels = c("N" = "Non-School Day", "Y" = "School Day")) + #Renaming the x-axis 
  scale_color_manual(values = c("School Day" = "steelblue", "Non-School Day" = "red")) + #manually changing the colors of non-school and school categories
    
    
    
  theme_light(base_size = 16) + #Changes base theme and also makes it have larger font 
  theme(panel.grid = element_blank(), #Customizing the background, removing gridlines 
        panel.background = element_rect(fill = "white",color = NA),
        plot.background = element_rect(fill = "white", color = NA),
        legend.position = "none") #Hides the legend 

```

#### c. Caption (5 points)

**Figure 1. Average time spent cooking(min) tends to be higher on School Days.** Data collected personally by Alexa Andrade. The points represent observations of daily time spent cooking(min) and are differentiated by whether the observations were taken on school(n=24) or non-school(n=10) days(total n=34). Colors represent the status of the day(red = non-school day, blue = school day). The graph represents the comparison between mean time spent cooking on different types of days, represented by the large black point, ± standard error(SE) bar whiskers between school days and non-school days.

#### d. Table presentation (10 points)

```{r Table}

Cooking_summary <- Cooking_summary |> 
  mutate(school_day = recode(school_day, #Renaming the categories in school_day column 
                             "N" = "Non-School Day",
                             "Y" = "School Day"))

Cooking_summary |>
  flextable() |> #Creating a summary statistics table 
  set_header_labels( #Renaming column headers for display
    school_day = "Status of Day",
    mean = "Mean",
    sd = "Standard Deviation",
    se = "Standard Error",
    ci_lower = "95% CI Lower",
    ci_upper = "95% CI Upper" ) |>
  set_caption("Summary Statistics of Cooking Time by School Day") |> #Adding a title to the table 
  autofit() #Adjusts column widths to fit content 

```

### Problem 2. Affective visualization (24 points)

#### a. Describe in words what an affective visualization could look like for your personal data (3-5 sentences). (2 points)

For my personal data I was inspired by Jill Pelto's "Seabirds of Seal Island" data visualization, so I also want to do a scenic affective visualization. I want to have the setting be a kitchen, and have two different countertops that represent two different approaches to preparing food for the day, one where their is a microwave and airfryer, and the other where there is a stove with cooking appliances. Between the two stoves, there is going to be shelves that are filled with a different number of ingredients and a differently sized spoon next to them to symbolize the proportion of dirty dishes used. Each cooking appliance on the stoves will be next to or hold a bowl that is varying degrees of filled, representing the amount of time that would typically be spent cooking using that appliance.

#### b. Create a sketch (on paper) of your idea. (2 points)

![Rough Sketch of Visualization](Sketch.png)

#### c. Make a draft of your visualization. (12 points)

![Draft of Visualization](Draft.png)

#### d. Write an artist statement. (8 points)

I chose to display the overall variables of my data collection in a scenic and symbolic way to give viewers an overall general understanding that the goal of my data was to observe how much time I spend in the kitchen. The bowls next to the kitchen appliances are filled in varying levels to represent how much time I would have to dedicate to cooking if I was using that specific appliance. The shelves are each filled with a different number of ingredients that would go into certain meals,and are accompanied by either a small, medium, or large spoon, which represent the degree of dirty dishes that would come with cooking that meal. I was heavily influenced by Jill Pelto's art, especially because all of her pieces seem like scenic art unitl the viewer takes the time to look at the details. Even then, Jill does not place a heavy emphasis on simply portraying the data, but focuses more on how that data can be contextualized using the scene around it. I decided to create my affective visualization using colored pencils, after sketching it out initially with pencil, because it allowed me to fully customize the visualization however I needed. I used colored pencils to display ow the variables I measured were: Minutes Spent Cooking, Number of dishes Dirtied, and Number of ingredients used. I started data collection approximetly half way through Spring quarter, looked at the overall outcomes of my data, and then drew and colored this affective visualization to communicate the theme of my data and its outcome.

### Problem 3. Statistical critique (36 points)

#### a. Revisit and summarize (6 points)

A two-tailed Student’s t-test was conducted to answer the author's main research question: Could mammalian cells retain a faithful copy of epigenetic information from earlier in life, which could serve as a guide in reversing aging? The response variable was axon regeneration and axon density, while the predictor variable was drug treatment of intravitreal injection of Adeno-Associated-Viruses that delivered OSK genes into the mice.The two-tailed student t-test was used to compare axon density and retinal ganglion counts between OSK gene-treated groups, so a significant result of a p-value less than 0.05 would mean that the OSK treatment led to reliable regeneration of axon density, supporting the paper’s hypothesis that epigenetic reprogramming can help reverse vision loss.

### Figure: Axon Density and Regeneration

![Axon density and regeneration](Axon_density.png) 

#### b. Visual clarity (10 points)

The authors did a clear job of logically labeling their axis and different control and treatment groups with color and logically, even including a legend to bring clarity of the different treatments. However, the positioning of the p-values above the graph makes it unclear which p-value is for which comparison. The figure does show mean with Standard error of the mean bars, with the underlying data points being displayed for visual clarity.

#### c. Aesthetic clarity (10 points)

I believe the authors handeled their visual clutter moderately well, as they ensured there were no background lines or grids that weren't actively symbolizing data. However, the data:ink ratio is not as maximized as it could be, with the P-values from the comparisons between the groups being unnecessarily underlined and larger than what I think is reasonable, adding more ink to the figure while not adding more information about the data.

#### d. Recommendations (can be longer than 4 sentences, 10 points)

To improve the aesthetic clarity of the figure, I recommend reducing the visual clutter by removing the underlining under the p-values. While I'm sure the underlines we're added to emphasize the p-values, I think that because the p-values are already located in a empty space and the underlines don't convey any statistical information, that removing them would be improve the data:ink ratio of the figure. I also recommend making the font size of the p-values smaller and adding subtle brackets labeling between the columns, so that it's more understandable on which p-values are for which group comparisons. All of this would lead to higher visual clarity and prevent ambiguity about what is being tested.
